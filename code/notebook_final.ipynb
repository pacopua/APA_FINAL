{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03da763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c60193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from class_imbalance import ImbalanceHandler\n",
    "\n",
    "# Configurar semilla para reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. CARGA Y PREPROCESAMIENTO\n",
    "# ==========================================\n",
    "\n",
    "# Cargar datos\n",
    "notebook_path = os.getcwd()\n",
    "file_path = Path(notebook_path) / \"../dataset/robot_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89a77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Estrategia 1: Resampling (Upsampling) ---\n"
     ]
    }
   ],
   "source": [
    "# Definir Target: 1 si hay grip_lost O ProtectiveStop\n",
    "df['target'] = ((df['grip_lost'] == True) | (df['Robot_ProtectiveStop'] == True)).astype(int)\n",
    "\n",
    "# Seleccionar Features\n",
    "feature_cols = [c for c in df.columns if c not in ['Num', 'Timestamp', 'cycle', 'grip_lost', 'Robot_ProtectiveStop', 'target']]\n",
    "X = df[feature_cols].values\n",
    "y = df['target'].values\n",
    "\n",
    "# Split Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Escalar Datos (Muy importante para redes neuronales)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertir a Tensores de PyTorch\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1) # Shape (N, 1)\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Modelo Base Simple (Perceptrón Multicapa)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1) # Salida lineal (Logits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x) # No aplicamos Sigmoid aquí, lo hará la función de pérdida\n",
    "        return x\n",
    "\n",
    "# Función auxiliar de entrenamiento\n",
    "def train_torch_model(model, criterion, optimizer, X, y, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "# Función auxiliar de evaluación\n",
    "def evaluate_torch_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test)\n",
    "        probs = torch.sigmoid(logits) # Aplicamos Sigmoid para obtener probabilidad 0-1\n",
    "        preds = (probs > 0.5).float()\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# ==========================================\n",
    "# ESTRATEGIA 1: RESAMPLING (Oversampling)\n",
    "# ==========================================\n",
    "print(\"\\n--- Estrategia 1: Resampling (Upsampling) ---\")\n",
    "\n",
    "## aqui llamamos a class_imbalance.py\n",
    "imbalance_handler = ImbalanceHandler()\n",
    "X_train_res, y_train_res = imbalance_handler.smote_data(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17515a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_res = SimpleNN(input_dim)\n",
    "# Usamos pérdida estándar porque los datos ya están balanceados artificialmente\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model_res.parameters(), lr=0.001)\n",
    "\n",
    "train_torch_model(model_res, criterion, optimizer, X_train_res, y_train_res)\n",
    "evaluate_torch_model(model_res, X_test_t, y_test_t)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# ESTRATEGIA 2: CLASS-WEIGHTED LOSS\n",
    "# ==========================================\n",
    "print(\"\\n--- Estrategia 2: Class Weighted Loss ---\")\n",
    "\n",
    "# 1. Calcular el peso: (Num Negativos / Num Positivos)\n",
    "num_neg = (y_train == 0).sum()\n",
    "num_pos = (y_train == 1).sum()\n",
    "pos_weight_val = num_neg / num_pos\n",
    "pos_weight_tensor = torch.tensor([pos_weight_val], dtype=torch.float32)\n",
    "\n",
    "print(f\"Peso calculado para clase positiva: {pos_weight_val:.2f}\")\n",
    "\n",
    "model_cw = SimpleNN(input_dim)\n",
    "# 2. Pasar el peso a la función de pérdida\n",
    "criterion_cw = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer_cw = optim.Adam(model_cw.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamos con los datos ORIGINALES (desbalanceados), el peso hace el trabajo\n",
    "train_torch_model(model_cw, criterion_cw, optimizer_cw, X_train_t, y_train_t)\n",
    "evaluate_torch_model(model_cw, X_test_t, y_test_t)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# ESTRATEGIA 3: FOCAL LOSS\n",
    "# ==========================================\n",
    "print(\"\\n--- Estrategia 3: Focal Loss ---\")\n",
    "\n",
    "# Definición de la clase Focal Loss en PyTorch\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: Logits (salida directa de la capa lineal)\n",
    "        # targets: Etiquetas verdaderas (0 o 1)\n",
    "        \n",
    "        # Calcular Binary Cross Entropy (sin reducción para poder operar)\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Calcular p_t (probabilidad de la clase verdadera)\n",
    "        # pt = exp(-BCE) es un truco numérico estable porque BCE = -log(pt)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        \n",
    "        # Fórmula Focal Loss: alpha * (1-pt)^gamma * BCE\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "model_fl = SimpleNN(input_dim)\n",
    "# alpha: Factor de balanceo. Si la clase 1 es minoritaria, usa un alpha alto (ej. 0.75 o 0.9)\n",
    "criterion_fl = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "optimizer_fl = optim.Adam(model_fl.parameters(), lr=0.001)\n",
    "\n",
    "train_torch_model(model_fl, criterion_fl, optimizer_fl, X_train_t, y_train_t)\n",
    "evaluate_torch_model(model_fl, X_test_t, y_test_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apa-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
